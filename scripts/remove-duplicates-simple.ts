#!/usr/bin/env tsx

import { createClient } from '@supabase/supabase-js';

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const serviceRoleKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !serviceRoleKey) {
  console.error('‚ùå Missing required environment variables:');
  console.error('   NEXT_PUBLIC_SUPABASE_URL:', !!supabaseUrl);
  console.error('   SUPABASE_SERVICE_ROLE_KEY:', !!serviceRoleKey);
  process.exit(1);
}

const supabase = createClient(supabaseUrl, serviceRoleKey);

async function removeDuplicates() {
  console.log('üßπ Starting RAG data duplicate removal...');
  
  try {
    // Get all records
    console.log('üìä Fetching all records...');
    const { data: allRecords, error: fetchError } = await supabase
      .from('knowledge_base')
      .select('*')
      .order('created_at', { ascending: true });

    if (fetchError) {
      console.error('‚ùå Error fetching records:', fetchError);
      return;
    }

    console.log(`üìã Found ${allRecords?.length || 0} total records`);

    // Group by content and identify duplicates
    const contentMap = new Map();
    const duplicateIds: string[] = [];

    allRecords?.forEach(record => {
      if (contentMap.has(record.content)) {
        // This is a duplicate - add to deletion list
        duplicateIds.push(record.id);
      } else {
        // This is the first occurrence - keep it
        contentMap.set(record.content, record);
      }
    });

    console.log(`üéØ Found ${duplicateIds.length} duplicate records to remove`);
    console.log(`‚úÖ Keeping ${contentMap.size} unique records`);

    if (duplicateIds.length === 0) {
      console.log('‚ú® No duplicates found!');
      return;
    }

    // Delete duplicates in batches of 100
    const batchSize = 100;
    let deletedCount = 0;

    for (let i = 0; i < duplicateIds.length; i += batchSize) {
      const batch = duplicateIds.slice(i, i + batchSize);
      console.log(`üóëÔ∏è Deleting batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(duplicateIds.length / batchSize)} (${batch.length} records)...`);

      const { error: deleteError } = await supabase
        .from('knowledge_base')
        .delete()
        .in('id', batch);

      if (deleteError) {
        console.error('‚ùå Error deleting batch:', deleteError);
        break;
      }

      deletedCount += batch.length;
      console.log(`‚úÖ Deleted ${deletedCount}/${duplicateIds.length} duplicate records`);
    }

    // Verify final state
    console.log('üîç Verifying final state...');
    const { data: finalRecords } = await supabase
      .from('knowledge_base')
      .select('content');

    const finalTotal = finalRecords?.length || 0;
    const finalUnique = new Set(finalRecords?.map(r => r.content)).size;

    console.log(`üìä Final result: ${finalTotal} total records, ${finalUnique} unique content`);
    
    if (finalTotal === finalUnique) {
      console.log('üéâ Success! All duplicates removed!');
    } else {
      console.log('‚ö†Ô∏è Warning: Some duplicates may still exist');
    }

    // Show breakdown by source
    console.log('üìà Final records by source:');
    const sources = new Map();
    finalRecords?.forEach(record => {
      // We need to get the source info
    });

    const { data: sourceStats } = await supabase
      .from('knowledge_base')
      .select('source')
      .then(result => {
        const stats = new Map();
        result.data?.forEach(record => {
          stats.set(record.source, (stats.get(record.source) || 0) + 1);
        });
        return { data: Array.from(stats.entries()) };
      });

    sourceStats?.data?.forEach(([source, count]) => {
      console.log(`  ${source}: ${count} records`);
    });

  } catch (error) {
    console.error('‚ùå Error during duplicate removal:', error);
  }
}

if (require.main === module) {
  removeDuplicates()
    .then(() => process.exit(0))
    .catch((error) => {
      console.error('‚ùå Failed:', error);
      process.exit(1);
    });
}