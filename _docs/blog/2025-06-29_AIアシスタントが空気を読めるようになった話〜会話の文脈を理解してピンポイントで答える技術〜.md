# AIアシスタントが空気を読めるようになった話〜会話の文脈を理解してピンポイントで答える技術〜

みんな、こんにちは！テリスケです。

最近、うちのAIアシスタント（エンジニアカフェのナビゲーター）が「空気を読めない」って苦情を受けたんよね（笑）

「カフェの営業時間を教えて」って聞いたら、メニューから価格から設備から、3000文字くらいダラダラと全部返してくる。いや、営業時間だけでいいのに！

これ、実は結構多くのAIアシスタントが抱えてる問題なんよ。今日はこの問題をどう解決したか、その実装の話をしようと思う。

## こんな人に読んでもらいたい

- RAG（Retrieval-Augmented Generation）を使ったAIアシスタントを作ってる人
- 会話の文脈を理解するシステムを実装したい人
- AIの回答が冗長すぎて困ってる人
- Next.js + TypeScriptでAIアプリを作ってる人
- 「空気読めよ」って言われがちな人（？）

## なぜこの問題に取り組んだのか

実はね、エンジニアカフェには2つのカフェがあるんよ。

1. **エンジニアカフェ**（コワーキングスペース）
2. **Saino Cafe**（併設のカフェ＆バー）

で、ユーザーが「カフェの営業時間は？」って聞いたときに、どっちのことか分からないから「どちらのカフェですか？」って聞き返すようにしてたんよね。

ここまでは良かった。

問題はその次。ユーザーが「あ、Saino Cafeの方で」って答えたときに、システムが暴走したんよ（笑）

```
ユーザー: カフェの営業時間を教えて
AI: コワーキングスペースのエンジニアカフェのことですか、それともSaino Cafeのことですか？
ユーザー: あ、Saino Cafeの方で
AI: [Saino Cafe Operating Hours] 併設のカフェ＆バー「saino」の営業時間は...（以下3000文字）
```

いや、営業時間だけでいいのに！メニューとか価格とか全部返してくるやん！

正直、これはかなり悔しかった。せっかく会話形式で聞き返してるのに、文脈を理解できてないんよね。

## 実装の全体像

まず、今回のシステムの技術スタックを整理しておこう。

| コンポーネント | 技術 | 役割 |
|------------|------|------|
| フロントエンド | Next.js 15 + TypeScript | UI全般 |
| AIフレームワーク | Mastra | エージェントの管理 |
| LLM | Google Gemini 2.5 Flash | 応答生成 |
| 音声認識/合成 | Google Cloud Speech | 音声インターフェース |
| ベクトルDB | PostgreSQL + pgvector | RAG検索 |
| 埋め込みモデル | Google text-embedding-004 | 知識ベースの検索 |

システムの流れはこんな感じ：

1. ユーザーが質問する
2. EnhancedQAAgentが質問を分析
3. 曖昧な場合は確認質問を返す
4. ユーザーの回答を受けて、RAG検索を実行
5. 検索結果から**必要な情報だけ**を抽出して返す

この「必要な情報だけ」ってところが今回のキモなんよね。

## 具体的な実装：会話の文脈を理解する

### 1. 特定のリクエストタイプを記憶する

まず、ユーザーが何を聞いたかを記憶する仕組みを作った。

```typescript
// 質問からリクエストタイプを抽出
private extractRequestTypeFromQuestion(question: string): string | null {
  if (question.includes('営業時間') || question.includes('何時') || 
      question.includes('hours') || question.includes('open')) {
    return 'hours';
  }
  if (question.includes('料金') || question.includes('価格') || 
      question.includes('いくら') || question.includes('price')) {
    return 'price';
  }
  // ... 他のタイプも同様に
  return null;
}
```

これで「営業時間」を聞いてることを覚えておけるようになった。

### 2. メモリシステムとの連携

次に、この情報をメモリに保存する。うちのシステムは3分間の短期記憶を持ってるんよ。

```typescript
// ユーザーの質問を記憶（リクエストタイプも含めて）
await this.simplifiedMemory.addMessage('user', question, {
  requestType: this.extractRequestTypeFromQuestion(question)
});
```

これがめっちゃ重要！後でフォローアップの質問が来たときに、最初に何を聞かれたか思い出せるんよ。

### 3. フォローアップ質問の処理

ここが一番悩んだところ（笑）

ユーザーが「あ、Saino Cafeの方で」って言ったときに、これがフォローアップ質問だって判定する必要がある。

```typescript
// フォローアップ質問かどうかチェック
const isFollowUp = question.includes('じゃ') || question.includes('では') || 
                   question.includes('の方は') || question.includes('then') || 
                   question.includes('how about');

// 前の質問で何を聞かれたか取得
const previousSpecificRequest = this.extractPreviousSpecificRequest(memoryContext);

if (isFollowUp && previousSpecificRequest) {
  // 営業時間だけを答えるように指示
  prompt = `ユーザーは以前${requestTypePrompt}について尋ね、今は特定の選択肢について聞いています。
           聞かれているものの${requestTypePrompt}のみを答えてください。
           重要：${requestTypePrompt}のみを答えてください。他の情報は含めないでください。最大1文。`;
}
```

これで、フォローアップ質問には必要な情報だけを返せるようになった！

### 4. RAG検索の改善

実は最初、RAG検索の結果がおかしかったんよね。Saino Cafeを聞いてるのに、エンジニアカフェの情報が先に出てきたり。

これは検索結果のソート方法に問題があった：

```typescript
// 修正前：重要度だけでソート
results.sort((a, b) => {
  const importanceOrder = { critical: 0, high: 1, medium: 2, low: 3 };
  return importanceOrder[a.metadata.importance] - importanceOrder[b.metadata.importance];
});

// 修正後：特定のクエリは類似度を優先
if (isSpecificQuery && Math.abs(a.similarity - b.similarity) > 0.2) {
  return b.similarity - a.similarity; // 類似度が高い方を優先
}
```

これで、Saino Cafeの情報がちゃんと上位に来るようになった。

## つまずいたポイントと解決方法

正直、かなり悩んだポイントがいくつかあった。

### 1. previousSpecificRequestがnullになる問題

最初、前の質問のリクエストタイプが取れなくて焦った。原因は単純で、メモリに保存するときにメタデータとして保存してなかったんよね。

```typescript
// ダメな例
await memory.addMessage('user', question);

// 良い例
await memory.addMessage('user', question, {
  requestType: 'hours' // これを追加！
});
```

### 2. レスポンスが長すぎる問題

RAG検索の結果をそのまま返すと、知識ベースのタグとか全部入ってきちゃう。

```typescript
// タグを削除
cleanedResponse = cleanedResponse.replace(/\[[^\]]+\]\s*/g, '');

// 長すぎる場合は要約
if (isSpecificRequest && cleanedResponse.length > 150) {
  const extractPrompt = `このテキストから特定の答えのみを抽出してください。最大1文にしてください: ${cleanedResponse}`;
  // 再度LLMで要約
}
```

### 3. 日本語と英語の混在

エンジニアカフェは国際的な場所なので、日英両対応が必要。これも地味に大変だった（笑）

## まとめ・終わりに

というわけで、AIアシスタントが「空気を読める」ようになった話でした。

実装を通じて学んだこと：
- **会話の文脈を保持することの重要性**：3分間の短期記憶でも十分効果的
- **特定の情報だけを抽出する技術**：プロンプトエンジニアリングが鍵
- **RAG検索の改善**：類似度と重要度のバランスが大事

失敗や改善点：
- まだ完璧じゃない。たまに余計な情報を返すことがある
- メモリの保持時間（3分）が適切かは要検証
- 複数回のフォローアップには対応できてない

でも、前よりずっと使いやすくなったと思う！

みんなも、AIアシスタントを作るときは「空気を読む」機能をぜひ実装してみてください。ユーザー体験が劇的に改善するよ！

今後は、もっと複雑な会話の流れにも対応できるようにしていきたいな。例えば「さっきの質問に戻るけど...」みたいなやつとか。

それでは、良いAI開発を！

P.S. ちなみに、この実装のおかげで、うちのAIアシスタントは「営業時間は11:00〜23:00です」って一文で答えるようになった。シンプル・イズ・ベスト！

---

*この記事で紹介したコードは、[engineer-cafe-navigator](https://github.com/terisuke/engineer-cafe-navigator)で公開してます。興味があったら覗いてみてね！*